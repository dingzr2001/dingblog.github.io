<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>AdaGrad and RMSProp Algorithm</title>
    <link href="/2023/02/18/AdaGrad-and-RMSProp-Algorithm/"/>
    <url>/2023/02/18/AdaGrad-and-RMSProp-Algorithm/</url>
    
    <content type="html"><![CDATA[<h1 id="AdaGrad-and-RMSProp-Algorithm"><a href="#AdaGrad-and-RMSProp-Algorithm" class="headerlink" title="AdaGrad and RMSProp Algorithm"></a>AdaGrad and RMSProp Algorithm</h1><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>Standard gradient descend is too scaled to be applied to modern deep neural network. AdaGrad is a effective method to optimize  by adjusting learning rate dynamically and automatically. </p><p>At each step,<br>$$<br>S_t&#x3D;S_{t-1}+\nabla W_t\cdot \nabla W_t<br>$$</p><p>$$<br>W_t &#x3D;W_{t-1}-\frac{\eta}{\sqrt{S_t+\epsilon}}\nabla W_t<br>$$</p><p>The more the weight vector has changed in a dimension,  the more gradient it has accumulated on that dimension.</p><p>In AdaGrad, we use $ S_t $ to count the gradient of W in the past, and update W with S. The bigger $ S_i $ is, the smaller the change on $ W_i $ would be. By doing this, we could adjust learning rate on each dimension respectively, according to the frequency and range of changes on each dimension of the weight vector. If your path has gone through a steep slope in the direction of X-axis, you need to slow down in X-axis in case you miss the terminal. But your path is quite flat in Y-axis, so you don’t need to slow down too much on that direction.</p><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>The problem of AdaGrad is that it counts on gradients in the past too much, so that it might not respond timely on the sudden change of gradient. That is because S has accumulated too much previous gradient. We want to reduce the impact of the gradient too far away in the past, so here we have RMSProp Algorithm:<br>$$<br>S_t&#x3D;\gamma S_{t-1}+(1-\gamma)\nabla W_t\cdot \nabla W_t<br>$$</p><p>$$<br>W_t &#x3D;W_{t-1}-\frac{\eta}{\sqrt{S_t+\epsilon}}\nabla W_t<br>$$</p><p>The only change is the weight $ \gamma $ of $ S_{t-1} $. This makes the portion of previous gradients in current S decay in a growing rate. </p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p><a href="https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf">https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/72039430">https://zhuanlan.zhihu.com/p/72039430</a></p><p><a href="https://blog.csdn.net/rpsate/article/details/127320124">https://blog.csdn.net/rpsate/article/details/127320124</a></p><p><a href="https://www.bilibili.com/video/BV1r64y1s7fU?spm_id_from=333.880.my_history.page.click">https://www.bilibili.com/video/BV1r64y1s7fU?spm_id_from=333.880.my_history.page.click</a></p>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>Dive into Deep Learning-Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>optimization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2023/02/05/CNN%20Convolution/"/>
    <url>/2023/02/05/CNN%20Convolution/</url>
    
    <content type="html"><![CDATA[<hr><hr><h1 id="CNN-Convolution"><a href="#CNN-Convolution" class="headerlink" title="CNN: Convolution?"></a>CNN: Convolution?</h1><p>Now I am at chapter <em>convolutional neural network</em>. There is a formula with many subscripts:</p><p><img src="D:\dingblog\source\img\convolution\formula.png"></p><p>What does it mean?</p><p>Well, let’s put it in a graph. </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Covariate Shift, Label Shift and Concept Shift</title>
    <link href="/2023/01/30/%5BDive%20into%20Deep%20Learning%5D%20Covariate%20Shift,%20Label%20Shift%20and%20Concept%20Shift/"/>
    <url>/2023/01/30/%5BDive%20into%20Deep%20Learning%5D%20Covariate%20Shift,%20Label%20Shift%20and%20Concept%20Shift/</url>
    
    <content type="html"><![CDATA[<h1 id="Dive-into-Deep-Learning-Covariate-Shift-Label-Shift-and-Concept-Shift"><a href="#Dive-into-Deep-Learning-Covariate-Shift-Label-Shift-and-Concept-Shift" class="headerlink" title="[Dive into Deep Learning] Covariate Shift, Label Shift and Concept Shift"></a>[Dive into Deep Learning] Covariate Shift, Label Shift and Concept Shift</h1><p>When studying distribution shift, these concepts kept confusing me for a long time. After searching they finally become clearer.</p><h2 id="Covariate-Shift"><a href="#Covariate-Shift" class="headerlink" title="Covariate Shift"></a>Covariate Shift</h2><p>First, what exactly is a <strong>covariate</strong> in machine learning? Well, after searching I found that covariates in statistics corresponds with features in machine learning, which means that covariate shift is actually the shift of features. In many cases, the features of test datasets might differ from those of the training data sets, yet the labels remain the same. For example, one model is trained with a set of real dog pictures, but it might be required to distinguish a comic dog like Snoopy. The model has never seen Snoopy-like cartoon dogs before, and such dogs have different distribution of features and labels with real dogs, but they share the same label: dog. Such a shift of distribution is called <strong>covariate shift</strong>.</p><h2 id="Label-Shift"><a href="#Label-Shift" class="headerlink" title="Label Shift"></a>Label Shift</h2><p>Label shift is exactly the opposite of covariate shift. Here is what I found at <strong><a href="https://towardsdatascience.com/">https://towardsdatascience.com</a></strong>, written(or quoted) by Dr. Matthew Stewart:</p><blockquote><p>Prior probability shift appears only in Y-&gt;X problems, and is defined as the case where P<del>tra</del>(x|y) &#x3D; P<del>tst</del>(x|y) and P<del>tra</del>(y) ≠ P<del>tst</del>(y).</p></blockquote><p>So therefore the example of diseases and symptoms makes sense. We want to predict disease by symptoms, so ‘symptoms’ is X, and ‘diseases’ is Y. Apparently diseases cause symptoms, so this is a typical Y-&gt;X case. The distribution of diseases is not always the same, since diseases occur at different rates in different seasons, so the distribution of Y changes. This is a label shift.</p><h2 id="Concept-Shift"><a href="#Concept-Shift" class="headerlink" title="Concept Shift"></a>Concept Shift</h2><p>Concept shift is different, it is the mere change of P(y|x). Let’s assume there was once a type of medicine, but it was later redefined as a type of beverage by FDA, though its ingredients remains the same. So a model developed at the earlier period trying to classify medicine and beverage by the ingredients may not work well later.</p><p>That is my preliminary understanding of distribution shifts in this book. There’s still some ambiguity though, and I’ll keep working on that.</p>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>Dive into Deep Learning-Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>distribution shift</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/09/28/hello-world/"/>
    <url>/2022/09/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
